Below is a focused **PRD for a Cited Web Research Assistant powered by Serper** (Google Search API), tailored to your ADK repo and the “citations‑first, conservative planning” spec you shared. I’ve aligned deliverables to your existing scaffolding (agent wiring, mock tool replacement, onboarding, and evaluation flow).    

> **Security note:** The snippet you pasted includes a plaintext API key. If it’s real, **rotate it now** and store the new key in `my_agent/.env`. Never commit API keys to git. (Your onboarding flow already uses `.env`—we’ll reuse it.) 

---

# PRD — Cited Web Research Assistant (Serper‑powered)

## 1) Overview

Build a **web‑searching assistant** that prioritizes **transparent retrieval, citations‑first answers, and conservative multi‑step planning**, using **Serper** as the search backend. The product ships inside your **Google ADK** project by replacing the mock `web_search` tool with a real Serper integration, and by adding a thin orchestration layer that fetches pages, extracts quotes, and composes source‑backed answers. It runs in your existing **ADK web UI** and **evaluation harness**.   

**Why now:** The hackathon requires an agent that can search the web, read files, reason step‑by‑step, and handle multiple modalities. This PRD covers the **search pillar** with robust citations and planning, and fits your judging criteria (accuracy, speed, presentation). 

---

## 2) Goals (v1)

* **Citations‑first answers**: Every non‑trivial claim has **≥2 independent sources**; prefer **primary** documents (gov, IR, standards).
* **Transparent browsing**: Show the **queries used**, what was **fetched**, and **dates/domains** for each source.
* **Step‑wise planning**: Deterministic, budgeted loop (search → fetch → verify → compose) with clear termination conditions.
* **Short verbatim quotes**: ≤120 chars per quote with **publication date** and **link** (and **page/timestamp** for PDFs/videos).
* **Low hallucination**: Conservative synthesis; explicitly mark uncertainty / disagreement.

**Non‑Goals (v1)**
Portfolio tracking or trading actions; building our own web index; paywall circumvention.

---

## 3) Users & Jobs

* **Analysts & builders** needing fast, reliable, **source‑backed** answers.
* **PMs/engineers** wanting reproducible research logs.
* **General users** valuing clean summaries with links to dig deeper.

Primary jobs:

1. “Give me a current answer with **citations I can trust**.”
2. “Verify a headline against **primary sources**.”
3. “Summarize a doc/video and **quote exact lines**.”

---

## 4) Core Flows

### A. Quick Answer (default)

* **Budgets**: `max_searches=3`, `max_fetches=5`, `min_sources=2`.
* **Steps**: search → rank by domain quality → fetch → extract **2–4 short quotes** → synthesize with inline citations.
* **Termination**: (a) min_sources met, (b) diminishing returns after last hop, or (c) budget hit.

### B. Deep Check (escalated)

* Trigger on ambiguous/controversial questions or when sources conflict.
* **Budgets**: `max_searches=5`, `max_fetches=8`, `min_sources=3`, `require_primary=True`.
* Output adds **Claim vs Evidence** bullets and an explicit **confidence** rating.

### C. File‑augmented (v0.2+)

* If a PDF or YouTube link is present, **quote the original** with **page/timestamp**, then **cross‑check** via web search.

> The ADK web UI remains the primary UX for development and demo (`uv run adk web`). 

---

## 5) Functional Requirements

### 5.1 Tools (minimum v1)

* **web_search(query, recency_days?, sites?, blocked_sites?, max_results=10) → {results[]}**

  * **Backend**: Serper `https://google.serper.dev/search`
  * Map fields to `{title, snippet, url, date?, domain}`.
* **fetch_url(url) → {title, text, published_at?, domain, url}**

  * Use Trafilatura/Readability for main content extraction (implementation detail).
* **compose_answer(...) → SearchAnswer JSON** (contract below)
* **(v0.2)** `fetch_pdf(path|url) → {pages[], meta}` for page‑aware quotes
* **(v0.3)** `extract_transcript(youtube_url) → {segments[], timestamps}`

> Replace the current **mock** tool in `my_agent/tools/web_search.py` and register the real tool in `my_agent/agent.py`. Keep the return schema for minimal refactors.  

### 5.2 Orchestration (planner)

* Generate **1–3 queries** → Serper search → rank by **source quality tier** and **recency** → fetch **top N** → extract **quotes** → compose answer.
* **Verification rules**:

  * Facts require **≥2 sources** unless a **definitive primary** exists.
  * Always surface **dates** near claims.
  * Penalize undated/low‑credibility domains.
* **Termination**: min_sources met **and** quality threshold satisfied; otherwise escalate to Deep Check.

### 5.3 Output Contract (all flows)

```json
{
  "summary": "string",
  "bullets": ["string", "..."],
  "quotes": [{"text":"...", "source":"...", "url":"...", "date":"YYYY-MM-DD", "page_or_ts":"p.12|01:23"}],
  "sources": [{"title":"...", "domain":"...", "url":"...", "date":"YYYY-MM-DD"}],
  "method": {"queries":["..."], "hops": 3, "notes":"..."},
  "confidence": "low|medium|high"
}
```

* **UI copy**:

  * Top line **summary**, then **bullets** with inline `[domain · date]`, then **Quotes** (monospace), **Sources** list, and a **How I searched** footnote:
    *Searched X, fetched Y, verified against Z.*

---

## 6) Non‑Functional Requirements

* **Latency**: Quick ≤8s P50 / ≤15s P95; Deep Check ≤25s P95.
* **Reliability**: Retries with backoff; skip blocked/paywalled pages; degrade gracefully with partial citations.
* **Observability**: Structured logs of queries, domains, fetch timings, decisions (redacted).
* **Privacy & Security**: No content persistence by default; redact PII; **store API keys in `my_agent/.env`** (not in code). 

---

## 7) API & Data Contracts

### 7.1 Serper usage (productionized)

**Endpoint:** `POST https://google.serper.dev/search`
**Headers:** `X-API-KEY: $SERPER_API_KEY`, `Content-Type: application/json`
**Body:** `{"q": "<query>", "num": 10, "tbs": "qdr:30"}` (optional **recency_days** → `qdr:N`)
**Mapping to internal result:**

```json
{
  "title": "<item.title>",
  "snippet": "<item.snippet>",
  "url": "<item.link>",
  "date": "<item.date || item.dateUtc || ''>",
  "domain": "<parsed hostname>"
}
```

**Tool signature (Python):**

```python
def web_search(
  query: str,
  recency_days: int | None = None,
  sites: list[str] | None = None,
  blocked_sites: list[str] | None = None,
  max_results: int = 10
) -> dict  # {"results":[...], "query": "..."}
```

**Config**

* `SERPER_API_KEY` in `my_agent/.env` (not committed); respect rate limits; add 429/5xx retry w/ jitter. 

### 7.2 Answer object

* See **Output Contract** (Section 5.3). TypedDict optional for dev ergonomics.

---

## 8) Source Quality Heuristics

* **Tier 1 (primary):** gov & standards (e.g., `sec.gov`, `ec.europa.eu`, `fda.gov`), company IR.
* **Tier 2:** top‑tier journalism (Reuters, FT, WSJ, NYT, Bloomberg).
* **Tier 3:** other outlets, blogs; penalize **undated** or **syndicated** duplicates.
* Ranking key: `(-tier_score, recency, title match)`.

---

## 9) Error Handling & Degradation

* **Search failure**: retry 3× (exponential backoff), then return a partial with **uncertainty note**.
* **Fetch failure**: skip URL, continue until **min_sources** met or budgets exhausted.
* **Undated pages**: keep only if supported elsewhere; mark confidence **medium**.

---

## 10) Acceptance Criteria

1. For 20 diverse queries, answers include **≥2 sources** (≥1 primary when applicable).
2. Every quote has **date + link** (and **page/time** for PDFs/videos).
3. On disagreement, include a **conflict note** and **confidence ≤ medium**.
4. No undated/low‑credibility page is the **sole** support for any claim.
5. Meets **latency targets**.
6. Works inside your **ADK UI** and **evaluation script** without changes to the harness. 

---

## 11) Implementation Plan (repo‑aligned)

**P0 — Replace mock search + Quick flow**

1. **Serper tool**: Implement `my_agent/tools/web_search.py` using Serper and keep the same return schema currently expected by your agent. 
2. **Answer composer**: Add `fetch_url` + minimal `compose_answer` utilities in `my_agent/tools/` (small modules).
3. **Agent wiring**: In `my_agent/agent.py`, instruct the root agent to call `web_search` first, then fetch/compose. Keep model as `gemini-flash-lite-latest`. 
4. **Secrets**: Put `SERPER_API_KEY` in `my_agent/.env`. Use your onboarding commands to run UI and tests. 

**P1 — Deep Check + transparency**

* Add a `web_research_deep_check` tool with stricter budgets and **require_primary=True**.
* Show **method** block (queries, hops) in the final answer.

**P2 — File‑augmented**

* Add `fetch_pdf` (page‑aware) and optional YouTube transcript tool.
* Quote with `page_or_ts` and cross‑check via Serper.

**Developer workflow & demo:**

* **Run:** `uv run adk web` (chat & demo) and `uv run python evaluate.py` (metrics). 

---

## 12) Observability, Security, Cost

* **Logs:** query text (hashed for PII), domains, fetch timing, decision path, status codes.
* **Metrics:** P50/P95 latency, sources/answer, primary ratio, quote validity rate.
* **Secrets:** keys only in `.env`; never printed or returned to users. 
* **Cost:** Cap `max_searches`/`max_fetches`; add basic domain blocklist; cache identical queries per session.

---

## 13) Rollout Plan

* **v0.1** (Today): Serper‑backed **Quick Answer** flow, citations + quotes.
* **v0.2**: **Deep Check** mode, source‑scoring + “How I searched” section.
* **v0.3**: **File‑augmented** (PDF/YouTube) with page/timestamp quotes.

---

## 14) Open Questions

* Domain allow/deny presets per topic (finance/science/health)?
* When to **auto‑escalate** to Deep Check (keyword list vs. disagreement heuristic)?
* Minimum **quote count** per answer (2 vs. 3) vs. latency budget trade‑off?

---

## 15) Appendix — Minimal Serper integration (safe pattern)

```python
# my_agent/tools/web_search.py
import os, json, requests
from urllib.parse import urlparse

SERPER_API_KEY = os.getenv("SERPER_API_KEY")  # set in my_agent/.env

def web_search(query: str,
               recency_days: int | None = None,
               sites: list[str] | None = None,
               blocked_sites: list[str] | None = None,
               max_results: int = 10) -> dict:
    """
    Performs Serper search and returns normalized results.
    """
    headers = {"X-API-KEY": SERPER_API_KEY, "Content-Type": "application/json"}
    q = query
    if sites:
        q += " " + " OR ".join([f"site:{s}" for s in sites])
    payload = {"q": q, "num": max_results}
    if recency_days:
        payload["tbs"] = f"qdr:{recency_days}"  # 'past N days'

    r = requests.post("https://google.serper.dev/search", headers=headers, data=json.dumps(payload), timeout=20)
    r.raise_for_status()
    data = r.json()

    results = []
    for item in data.get("organic", []):
        url = item.get("link", "")
        domain = urlparse(url).netloc
        if blocked_sites and any(b in domain for b in blocked_sites):
            continue
        results.append({
            "title": item.get("title"),
            "snippet": item.get("snippet"),
            "url": url,
            "date": item.get("date") or item.get("dateUtc") or "",
            "domain": domain
        })
    return {"results": results, "query": query}
```

> Register this tool in `my_agent/agent.py` and keep using your ADK UI + evaluation flow as documented.  

---

**This PRD gives Claude Code exactly what it needs**: replace the mock search with Serper, add deterministic planning and structured outputs, keep secrets in `.env`, and ship a credible “citations‑first” demo that fits your hackathon scaffolding and judging rubric.   
